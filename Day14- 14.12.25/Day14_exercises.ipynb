{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nh8ucFzy_RGk",
        "outputId": "2ec6d030-cfdb-494a-a790-7a16c86cae19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9.9 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "      .appName(\"Exercises\") \\\n",
        "      .getOrCreate()"
      ],
      "metadata": {
        "id": "Mg3zLosr_9m7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "(\"C001\", \"Arjun\", \"Hyderabad\", 25, 45000, \"Electronics\"),\n",
        "(\"C002\", \"Meera\", \"Chennai\", 32, 52000, \"Grocery\"),\n",
        "(\"C003\", \"Rajesh\", \"Bangalore\", 29, 61000, \"Clothing\"),\n",
        "(\"C004\", \"Priya\", \"Delhi\", 22, 38000, \"Grocery\"),\n",
        "(\"C005\", \"Sanjay\", \"Mumbai\", 35, 72000, \"Electronics\"),\n",
        "(\"C006\", \"Kavya\", \"Hyderabad\", 28, 48000, \"Grocery\"),\n",
        "(\"C007\", \"Imran\", \"Delhi\", 31, 53000, \"Clothing\"),\n",
        "(\"C008\", \"Divya\", \"Chennai\", 27, 45000, \"Electronics\"),\n",
        "(\"C009\", \"Anil\", \"Bangalore\", 40, 85000, \"Furniture\"),\n",
        "(\"C010\", \"Ritu\", \"Mumbai\", 23, 39000, \"Clothing\"),\n",
        "(\"C011\", \"Hari\", \"Hyderabad\", 33, 56000, \"Grocery\"),\n",
        "(\"C012\", \"Sana\", \"Delhi\", 26, 47000, \"Electronics\"),\n",
        "(\"C013\", \"Vikram\", \"Chennai\", 38, 91000, \"Furniture\"),\n",
        "(\"C014\", \"Deepa\", \"Mumbai\", 30, 62000, \"Clothing\"),\n",
        "(\"C015\", \"Asha\", \"Bangalore\", 24, 41000, \"Grocery\"),\n",
        "(\"C016\", \"Kiran\", \"Delhi\", 29, 59000, \"Furniture\"),\n",
        "(\"C017\", \"Farah\", \"Hyderabad\", 36, 70000, \"Clothing\"),\n",
        "(\"C018\", \"Tarun\", \"Chennai\", 28, 53000, \"Furniture\"),\n",
        "(\"C019\", \"Nisha\", \"Mumbai\", 21, 35000, \"Grocery\"),\n",
        "(\"C020\", \"Yusuf\", \"Bangalore\", 34, 76000, \"Electronics\"),\n",
        "(\"C021\", \"Pooja\", \"Delhi\", 27, 47000, \"Clothing\"),\n",
        "(\"C022\", \"Zara\", \"Hyderabad\", 32, 58000, \"Grocery\"),\n",
        "(\"C023\", \"Ajay\", \"Chennai\", 30, 51000, \"Furniture\"),\n",
        "(\"C024\", \"Reema\", \"Bangalore\", 28, 49000, \"Clothing\"),\n",
        "(\"C025\", \"Gautam\", \"Mumbai\", 39, 82000, \"Furniture\"),\n",
        "(\"C026\", \"Swati\", \"Delhi\", 25, 46000, \"Electronics\"),\n",
        "(\"C027\", \"Mahesh\", \"Hyderabad\", 41, 90000, \"Furniture\"),\n",
        "(\"C028\", \"Anita\", \"Chennai\", 26, 44000, \"Clothing\"),\n",
        "(\"C029\", \"Sameer\", \"Bangalore\", 33, 68000, \"Electronics\"),\n",
        "(\"C030\", \"Leela\", \"Delhi\", 22, 36000, \"Grocery\")\n",
        "]\n",
        "\n",
        "columns = [\"customer_id\", \"name\", \"city\", \"age\", \"annual_spend\", \"category\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "id": "sPre1x0s_on5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 1  - show first 10 rows\n"
      ],
      "metadata": {
        "id": "Cc1oWOyRBKph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(10)"
      ],
      "metadata": {
        "id": "03u_LspzArwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise -2 Show the unique cities\n"
      ],
      "metadata": {
        "id": "rC5sB513BXU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"city\").distinct().show()"
      ],
      "metadata": {
        "id": "GFyH1Q8SA2eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise - 3 Display only customer_id, name, and annual_spend columns."
      ],
      "metadata": {
        "id": "WWLQtb83Bq8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"customer_id\" , \"name\" , \"annual_spend\").show()"
      ],
      "metadata": {
        "id": "iZ--ZpRmBv4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise - 4 Filter all customers who spend more than 60000 annually."
      ],
      "metadata": {
        "id": "9TRre_JaB8Gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df.annual_spend > 60000).show()"
      ],
      "metadata": {
        "id": "5vf6R_DpCYDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise - 5 Show all customers from Delhi who are younger than 30."
      ],
      "metadata": {
        "id": "4RRBbyqJCpot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter((df.city == \"Delhi\") & (df.age < 30)).show()"
      ],
      "metadata": {
        "id": "A8ORGBs0CxxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise - 6 Create a new column named \"spend_lakh\" = annual_spend / 100000."
      ],
      "metadata": {
        "id": "Yf_Lj1AUDJAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "df2 = df.withColumn(\"spend_lakh\" , col(\"annual_spend\") / 100000 )\n",
        "df2.show()"
      ],
      "metadata": {
        "id": "MHSBXLHCDOoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise - 7 Create a new column \"customer_type\"\n",
        "Logic:\n",
        "spend > 70000 → Premium\n",
        "else → Standard\n",
        "Use when() and otherwise()."
      ],
      "metadata": {
        "id": "ZifEQXShEAGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col , when\n",
        "df3 = df.withColumn(\"customer_type\" , when(df[\"annual_spend\"] > 7000 , \"Premium\").otherwise(\"Standard\")  )\n",
        "df3.show()"
      ],
      "metadata": {
        "id": "EAIDR30xEGLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise - 8 Show customers whose name starts with the letter A."
      ],
      "metadata": {
        "id": "IxukuvdhGjwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col,when\n",
        "df.filter(col(\"name\").startswith(\"A\")).show()"
      ],
      "metadata": {
        "id": "549lzoH5Gpvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise - 9 Filter customers where category is either Clothing or Electronics."
      ],
      "metadata": {
        "id": "yLYE6mvqHjv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter((df[\"category\"] == \"Clothing\") | (df[\"category\"] == \"Electronics\")).show()"
      ],
      "metadata": {
        "id": "AkCco83kHqt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise - 10 Convert the city name to uppercase using the upper() function."
      ],
      "metadata": {
        "id": "RATu_JroJSyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import upper , col\n",
        "df4 = df.withColumn(\"Uppered_cities\" , upper(col(\"city\")) )\n",
        "df4.show()"
      ],
      "metadata": {
        "id": "TZW5NMD4JZal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise - 11 Remove the category column from the DataFrame."
      ],
      "metadata": {
        "id": "ysFFkXtcKVYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_drop = df.drop(\"category\")\n",
        "df_drop.show()"
      ],
      "metadata": {
        "id": "gkDi3ocnKaX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise - 12 Sort customers by age in descending order."
      ],
      "metadata": {
        "id": "RY75K4yRKpQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sorted = df.orderBy(col('age').desc())\n",
        "df_sorted.show()"
      ],
      "metadata": {
        "id": "GxpE-ig3Kugy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise - 13 Find customers who are younger than the average age of the entire dataset.\n",
        "Steps:\n",
        "1. Calculate average age.\n",
        "\n",
        "2. Filter df where age < avg_age."
      ],
      "metadata": {
        "id": "1mm8UfCXTrXJ"
      }
    }
  ]
}