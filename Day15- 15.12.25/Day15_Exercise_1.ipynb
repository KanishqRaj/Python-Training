{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "        .appName(\"Day15_1\")\\\n",
        "        .getOrCreate()\n"
      ],
      "metadata": {
        "id": "BIhuxJKbTtPI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-rdau8GSFjk"
      },
      "outputs": [],
      "source": [
        "\n",
        "data = [\n",
        "    (\"O001\",\"Amit\",\"Hyderabad\",\"Spice Hub\",\"Indian\",450,35,\"UPI\",\"Delivered\"),\n",
        "    (\"O002\",\"Neha\",\"Bangalore\",\"Pizza Town\",\"Italian\",650,40,\"Card\",\"Delivered\"),\n",
        "    (\"O003\",\"Rahul\",\"Delhi\",\"Burger Zone\",\"American\",520,30,\"Cash\",\"Delivered\"),\n",
        "    (\"O004\",\"Pooja\",\"Mumbai\",\"Sushi Bar\",\"Japanese\",1200,55,\"UPI\",\"Cancelled\"),\n",
        "    (\"O005\",\"Arjun\",\"Chennai\",\"Curry Leaf\",\"Indian\",380,28,\"UPI\",\"Delivered\"),\n",
        "    (\"O006\",\"Sneha\",\"Hyderabad\",\"Pasta Street\",\"Italian\",700,45,\"Card\",\"Delivered\"),\n",
        "    (\"O007\",\"Karan\",\"Delhi\",\"Taco Bell\",\"Mexican\",540,33,\"UPI\",\"Delivered\"),\n",
        "    (\"O008\",\"Riya\",\"Bangalore\",\"Dragon Bowl\",\"Chinese\",600,38,\"Wallet\",\"Delivered\"),\n",
        "    (\"O009\",\"Vikas\",\"Mumbai\",\"BBQ Nation\",\"Indian\",1500,60,\"Card\",\"Delivered\"),\n",
        "    (\"O010\",\"Anjali\",\"Chennai\",\"Burger Zone\",\"American\",480,32,\"Cash\",\"Delivered\"),\n",
        "    (\"O011\",\"Farhan\",\"Delhi\",\"Biryani House\",\"Indian\",520,36,\"UPI\",\"Delivered\"),\n",
        "    (\"O012\",\"Megha\",\"Hyderabad\",\"Sushi Bar\",\"Japanese\",1100,58,\"Card\",\"Cancelled\"),\n",
        "    (\"O013\",\"Suresh\",\"Bangalore\",\"Curry Leaf\",\"Indian\",420,29,\"UPI\",\"Delivered\"),\n",
        "    (\"O014\",\"Divya\",\"Mumbai\",\"Pizza Town\",\"Italian\",780,42,\"Wallet\",\"Delivered\"),\n",
        "    (\"O015\",\"Nikhil\",\"Delhi\",\"Pasta Street\",\"Italian\",690,47,\"UPI\",\"Delivered\"),\n",
        "    (\"O016\",\"Kavya\",\"Chennai\",\"Dragon Bowl\",\"Chinese\",560,34,\"UPI\",\"Delivered\"),\n",
        "    (\"O017\",\"Rohit\",\"Hyderabad\",\"BBQ Nation\",\"Indian\",1400,62,\"Card\",\"Delivered\"),\n",
        "    (\"O018\",\"Simran\",\"Bangalore\",\"Burger Zone\",\"American\",510,31,\"Cash\",\"Delivered\"),\n",
        "    (\"O019\",\"Ayesha\",\"Mumbai\",\"Taco Bell\",\"Mexican\",570,35,\"UPI\",\"Delivered\"),\n",
        "    (\"O020\",\"Manish\",\"Delhi\",\"Curry Leaf\",\"Indian\",390,27,\"Wallet\",\"Delivered\"),\n",
        "    (\"O021\",\"Priya\",\"Hyderabad\",\"Pizza Town\",\"Italian\",720,41,\"Card\",\"Delivered\"),\n",
        "    (\"O022\",\"Yash\",\"Chennai\",\"Sushi Bar\",\"Japanese\",1150,57,\"UPI\",\"Delivered\"),\n",
        "    (\"O023\",\"Naina\",\"Bangalore\",\"Pasta Street\",\"Italian\",680,44,\"UPI\",\"Delivered\"),\n",
        "    (\"O024\",\"Sameer\",\"Mumbai\",\"Dragon Bowl\",\"Chinese\",610,39,\"Wallet\",\"Delivered\"),\n",
        "    (\"O025\",\"Ritika\",\"Delhi\",\"Burger Zone\",\"American\",500,30,\"Cash\",\"Delivered\"),\n",
        "    (\"O026\",\"Gopal\",\"Hyderabad\",\"Curry Leaf\",\"Indian\",410,28,\"UPI\",\"Delivered\"),\n",
        "    (\"O027\",\"Tina\",\"Bangalore\",\"Pizza Town\",\"Italian\",760,43,\"Card\",\"Delivered\"),\n",
        "    (\"O028\",\"Irfan\",\"Mumbai\",\"BBQ Nation\",\"Indian\",1550,65,\"Card\",\"Delivered\"),\n",
        "    (\"O029\",\"Sahil\",\"Chennai\",\"Taco Bell\",\"Mexican\",590,37,\"UPI\",\"Delivered\"),\n",
        "    (\"O030\",\"Lavanya\",\"Delhi\",\"Dragon Bowl\",\"Chinese\",630,40,\"Wallet\",\"Delivered\"),\n",
        "    (\"O031\",\"Deepak\",\"Hyderabad\",\"Burger Zone\",\"American\",520,33,\"Cash\",\"Delivered\"),\n",
        "    (\"O032\",\"Shweta\",\"Bangalore\",\"Curry Leaf\",\"Indian\",450,31,\"UPI\",\"Delivered\"),\n",
        "    (\"O033\",\"Aman\",\"Mumbai\",\"Pizza Town\",\"Italian\",810,46,\"Card\",\"Delivered\"),\n",
        "    (\"O034\",\"Rekha\",\"Chennai\",\"Pasta Street\",\"Italian\",700,45,\"UPI\",\"Delivered\"),\n",
        "    (\"O035\",\"Zubin\",\"Delhi\",\"BBQ Nation\",\"Indian\",1480,63,\"Card\",\"Delivered\"),\n",
        "    (\"O036\",\"Pallavi\",\"Hyderabad\",\"Dragon Bowl\",\"Chinese\",580,36,\"Wallet\",\"Delivered\"),\n",
        "    (\"O037\",\"Naveen\",\"Bangalore\",\"Taco Bell\",\"Mexican\",560,34,\"UPI\",\"Delivered\"),\n",
        "    (\"O038\",\"Sonia\",\"Mumbai\",\"Sushi Bar\",\"Japanese\",1180,59,\"Card\",\"Delivered\"),\n",
        "    (\"O039\",\"Harish\",\"Chennai\",\"Burger Zone\",\"American\",490,29,\"Cash\",\"Delivered\"),\n",
        "    (\"O040\",\"Kriti\",\"Delhi\",\"Curry Leaf\",\"Indian\",420,26,\"UPI\",\"Delivered\")\n",
        "]\n",
        "columns = [\n",
        "    \"order_id\",\"customer_name\",\"city\",\"restaurant\",\"cuisine\",\n",
        "    \"order_amount\",\"delivery_time_minutes\",\"payment_mode\",\"order_status\"\n",
        "]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "df.printSchema()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 1 - Write the full dataset to CSV with header enabled."
      ],
      "metadata": {
        "id": "G3icUAlCVg2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "with open (\"orders.csv\" , \"w\",newline ='') as f:\n",
        "  writer = csv.writer(f)\n",
        "  writer.writerow(columns)\n",
        "  writer.writerows(data)\n",
        ""
      ],
      "metadata": {
        "id": "IxiI9L0xVgE_"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_csv = spark.read\\\n",
        "          .option(\"header\",True)\\\n",
        "          .option(\"inferschema\",True)\\\n",
        "          .csv(\"orders.csv\")\n",
        "\n",
        "df_csv.show(truncate = False)\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "vCXY6yJIXzJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 2 - Read the CSV back and filter:\n",
        "order_amount > 700"
      ],
      "metadata": {
        "id": "oQimYcPsZl_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "df_csv = spark.read\\\n",
        "          .option(\"header\",True)\\\n",
        "          .option(\"inferschema\",True)\\\n",
        "          .csv(\"orders.csv\")\n",
        "\n",
        "df_filtered = df_csv.filter(F.col(\"order_amount\") > 700)\n",
        "df_filtered.show(truncate = False)"
      ],
      "metadata": {
        "id": "Awb_gYNTZs89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 3 - From CSV, show only:\n",
        "order_id\n",
        "city\n",
        "cuisine\n",
        "order_amount"
      ],
      "metadata": {
        "id": "ExQbNZbrfAa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_selected = df_csv.select(\"order_id\",\"city\",\"cuisine\",\"order_amount\")\n",
        "df_selected.show()"
      ],
      "metadata": {
        "id": "FY-5c5gKfJDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 4 - Sort orders by delivery_time_minutes descending and write result to CSV."
      ],
      "metadata": {
        "id": "ima-CZ2gf-65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "df_sorted = df_csv.orderBy(F.col(\"delivery_time_minutes\").desc())"
      ],
      "metadata": {
        "id": "l76Q-oNAgQ4w"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 5 - Write only “Delivered” orders to JSON."
      ],
      "metadata": {
        "id": "oW2Q4w-YnVfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "df_delivered = df.filter(\n",
        "    F.lower(F.trim(F.col(\"order_status\"))) == F.lit(\"delivered\")\n",
        ")\n",
        "# Write to JSON (folder): delivered_orders_json/\n",
        "(\n",
        "    df_delivered\n",
        "    .coalesce(1)                 # optional: single JSON part file\n",
        "    .write\n",
        "    .mode(\"overwrite\")           # overwrite if the folder exists\n",
        "    .json(\"delivered_orders_json\")\n",
        ")\n",
        "print(\"Delivered count:\", df_delivered.count())\n",
        "df_delivered.show(10, truncate=False)"
      ],
      "metadata": {
        "id": "E2xaQF7LlwzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 6\n",
        "Read JSON and filter:\n",
        "city = \"Mumbai\"\n",
        "payment_mode = \"Card\""
      ],
      "metadata": {
        "id": "djI4lYwax_k5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "df_filter = df.filter((F.lower(F.trim(F.col(\"city\"))) == F.lit(\"mumbai\")) & (F.lower(F.trim(F.col(\"payment_mode\"))) == F.lit(\"card\")) )\n",
        "df_filter.select(\n",
        "\"order_id\", \"customer_name\", \"city\", \"restaurant\",\n",
        "    \"cuisine\", \"order_amount\", \"delivery_time_minutes\",\n",
        "    \"payment_mode\", \"order_status\"\n",
        ").orderBy(F.col(\"order_amount\").desc()).show(truncate=False)\n",
        "print(\"Matching rows:\", df_filtered.count())"
      ],
      "metadata": {
        "id": "Khfg-tjTyGPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 7 - Add a column:\n",
        "delivery_category\n",
        "Logic:\n",
        "delivery_time_minutes > 45 → \"Late\"\n",
        "else → \"OnTime\""
      ],
      "metadata": {
        "id": "w9WCzvhVQTnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "df_1 = df.withColumn(\"delivery_category\" , F.when(F.col(\"delivery_time_minutes\") > 45, F.lit(\"Late\")).otherwise(F.lit(\"OnTime\")))\n",
        "(\n",
        "    df_1\n",
        "    .coalesce(1)                 # optional: single JSON part file\n",
        "    .write\n",
        "    .mode(\"overwrite\")\n",
        "    .json(\"orders_with_delivery_category_json\")\n",
        ")"
      ],
      "metadata": {
        "id": "vuKQJHsSQet_"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 8 - Force JSON output to a single partition and observe number of files created."
      ],
      "metadata": {
        "id": "iZQWqvb6R5H3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_base = df\n",
        "\n",
        "(\n",
        "    df_base.\n",
        "    coalesce(1).\n",
        "    write.\n",
        "    mode(\"overwrite\").\n",
        "    json(\"single_partition\")\n",
        ")\n",
        "df_base.show()"
      ],
      "metadata": {
        "id": "GGFavpXATkoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 9 - Convert full dataset to Parquet.\n",
        "Output:\n",
        "orders_parquet/"
      ],
      "metadata": {
        "id": "7qjqCGHAVIm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_parquet = df.write.mode(\"overwrite\").parquet(\"orders.parquet\")\n"
      ],
      "metadata": {
        "id": "okDI4kDSVO-5"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 10 - Read Parquet and filter:\n",
        "cuisine = \"Indian\"\n",
        "order_amount > 500"
      ],
      "metadata": {
        "id": "Z0vz3UxWV9iB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import functions as F\n",
        "df_parquet = spark.read.parquet(\"orders.parquet\")\n",
        "df_parquet = df_parquet.withColumn(\"order_amount\", F.col(\"order_amount\").cast(\"int\"))\n",
        "df_filtered = df_parquet.filter(\n",
        "    (F.col(\"cuisine\") == \"Indian\") & (F.col(\"order_amount\") > 500)\n",
        ")\n",
        "df_filtered.select(\n",
        "    \"order_id\", \"customer_name\", \"city\", \"restaurant\",\n",
        "    \"cuisine\", \"order_amount\", \"delivery_time_minutes\",\n",
        "    \"payment_mode\", \"order_status\"\n",
        ").orderBy(F.col(\"order_amount\").desc()).show(truncate=False)\n",
        "print(\"Matching row count:\", df_filtered.count())\n"
      ],
      "metadata": {
        "id": "-HmdCID0WHA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 11 - Sort Parquet data by order_amount descending and write top 10 orders back to Parquet."
      ],
      "metadata": {
        "id": "ViLVq-WBa8M8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top10_df=df_parquet.orderBy(\"order_amount\",ascending=False).limit(10)\n",
        "top10_df.write.mode(\"overwrite\").parquet(\"top10_orders_parquet\")\n",
        "top10_df.show()"
      ],
      "metadata": {
        "id": "5sy2FdJnbKTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 12 - Compare storage size of:\n",
        "CSV\n",
        "JSON\n",
        "Parquet\n",
        "Answer:\n",
        "Which is smallest?\n",
        "Why?"
      ],
      "metadata": {
        "id": "22-DYfIUgOBJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 13 - Convert:\n",
        "CSV → Parquet\n",
        "JSON → Parquet"
      ],
      "metadata": {
        "id": "Zv49k2EVgefU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "je2etCdCg0Ua"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}