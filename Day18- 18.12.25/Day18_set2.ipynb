{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sQ1v_usf3H2S"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "spark = SparkSession.builder.appName(\"DAY18\").getOrCreate()\n",
        "raw_users = [\n",
        "    (\"U001\",\"Amit\",\"28\",\"Hyderabad\",\"['AI','ML','Cloud']\"),\n",
        "    (\"U002\",\"Neha\",\"Thirty\",\"Delhi\",\"AI,Testing\"),\n",
        "    (\"U003\",\"Ravi\",None,\"Bangalore\",[\"Data\",\"Spark\"]),\n",
        "    (\"U004\",\"Pooja\",\"29\",\"Mumbai\",None),\n",
        "    (\"U005\",\"\", \"31\",\"Chennai\",\"['DevOps']\")\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 1 - Design an explicit schema using StructType"
      ],
      "metadata": {
        "id": "QNAr7mhJ5hZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "users_schema = StructType([\n",
        "    StructField(\"user_id\",StringType(),True),\n",
        "    StructField(\"name\",StringType(),True),\n",
        "    StructField(\"age\",StringType(),True),\n",
        "    StructField(\"city\",StringType(),True),\n",
        "    StructField(\"skills\",StringType(),True)\n",
        "])\n",
        "\n",
        "df_users = spark.createDataFrame(data=raw_users,schema=users_schema)\n",
        "df_users.printSchema()\n",
        "df_users.show()"
      ],
      "metadata": {
        "id": "Wkj6P-bp6NkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 2 - Normalize age into IntegerType"
      ],
      "metadata": {
        "id": "bzhreSEK6tFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import expr, col\n",
        "df_named = (\n",
        "    df_users\n",
        "    .withColumn(\"age_int\", expr(\"try_cast(age as int)\"))\n",
        ")\n",
        "df_failed = df_check.filter(col(\"age_int\").isNull())\n",
        "df_failed.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtNGWWQ97oFn",
        "outputId": "7b0891af-319f-4e21-efe9-3be40fcccaf1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+------+---------+-------------+-------+\n",
            "|user_id|name|   age|     city|       skills|age_int|\n",
            "+-------+----+------+---------+-------------+-------+\n",
            "|   U002|Neha|Thirty|    Delhi|   AI,Testing|   NULL|\n",
            "|   U003|Ravi|  NULL|Bangalore|[Data, Spark]|   NULL|\n",
            "+-------+----+------+---------+-------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 3 - Normalize skills into ArrayType"
      ],
      "metadata": {
        "id": "njzrxlfbANYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col, split, regexp_replace, coalesce, lit, array\n",
        "\n",
        "df_users1 = (\n",
        "    df_users\n",
        "    .withColumn(\n",
        "        \"skills\",\n",
        "        when(col(\"skills\").isNull(), array())  # empty array if NULL\n",
        "        .otherwise(\n",
        "            split(\n",
        "                regexp_replace(coalesce(col(\"skills\"), lit(\"\")), r\"[\\[\\]' ]\", \"\"),  # remove brackets/quotes/spaces\n",
        "                \",\"\n",
        "            )\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "df_users.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "0RecOlNfATuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 4 - Handle empty or missing names"
      ],
      "metadata": {
        "id": "HlarVGDVB9hK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import when\n",
        "df_named = df_users1.withColumn(\n",
        "    \"name_clean\",\n",
        "    when(col(\"name\").isNull() | (trim(col(\"name\")) == \"\"), lit(\"UNKNOWN\"))\n",
        "    .otherwise(col(\"name\"))\n",
        ")"
      ],
      "metadata": {
        "id": "PvttPdlnCE2t"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 5 - Produce a clean users_df"
      ],
      "metadata": {
        "id": "WKyFNbXsC0j8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "users_df = (\n",
        "    df_named\n",
        "    .select(\n",
        "        col(\"user_id\"),\n",
        "        col(\"name_clean\").alias(\"name\"),\n",
        "        col(\"age_int\").alias(\"age\"),\n",
        "        col(\"city\"),\n",
        "        col(\"skills_clean\").alias(\"skills\")\n",
        "    )\n",
        ")\n",
        "users_df.show(truncate=False)\n",
        "users_df.printSchema()\n"
      ],
      "metadata": {
        "id": "WFx_LsUDC8Sq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}